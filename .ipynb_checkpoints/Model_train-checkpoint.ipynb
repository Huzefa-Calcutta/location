{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection:\n",
    "Our output is column ‘label’ which is categorical variable having 2 classes 0 and 1. While selecting modeling technique to build the classifier we need to consider the following aspects:\n",
    "1.\tApproximates the training data as closely as possible.\n",
    "2.\tGeneralizable for unseen data (Avoids overfitting)\n",
    "3.\tHas good runtime performance i.e. it takes less time to run inference and occupies small disk size \n",
    "\n",
    "From the exploratory data analysis, in order to have good performance on training as well as test data the model we build needs to tackle:\n",
    "1.\tClass imbalance\n",
    "2.\tPresence of outliers in numerical variables\n",
    "3.\tCorrelation between the input variables\n",
    "4.\tVariables having values in different ranges i.e. variability of scale\n",
    "5.\tCategorical variables\n",
    "\n",
    "Considering these we choose two modeling techniques Random Forest and Support vector machine\n",
    "\n",
    "Random Forest is robust to overfitting as it fits large number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The number of trees can be increased till the model stops overfittings and generalzies pretty well to unseen data. Nowadays, fitting large number of trees is not computionally expensive because of easy accessibility parallel computing. Random forest can handle variables of different scales as it's based on tree partitioning alogorithm which is immune to the absoulte value of the variable. Only ranking of values matters. Random forest also handles outliers well because each tree is build on random subset of data. Multicollinearity is also well handled because for each split we randomly select only subset of variables. As far as class imbalance is concerned it can be handled by assigning more weights to the class with less frequency. In sklearn this is achieved by setting the parameter 'class_weight' equal to 'balanced' \n",
    "\n",
    "SVM is also robust to overfitting because it contains the cost parameter which acts as regularizer there by preventing overfitting.Cost paramenter can be fine-tuned to get optimum model performance on test data. As SVM uses hinge loss to get the optimum hyperplane separating the two classes, it is robust to outlier presence. The soft margin based hyperplane depends solely on few points known as support vectors which are close to the hyperplane. Other points don't have much of an impact on the optimum hyperplane. We can account for multicollinearity by adding regularization penalty to the hinge loss. Adding regularization term, the weights of variables which are artificially boosted as they are related to other variable which strongly impacts the outcome variable. SVM is sensitive to variable scale and it gives more weightage variables of higher magnitude. However, we can overcome this by first re-scaling all the variable so that all variables are in same range. In our case, since the numerical variables have outliers we use Robustscaler from sklearn to rescale data. As the name suggests Robustscaler is robust to presence of outliers unlike Minmax or Abs scaler which are very sensitive to outlier presence. Similar to SVM, we set parameter 'class_weight' equal to 'balanced' to tackle class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and do some basic data cleansing:\n",
    "1. Assigning rare classes in variable a_0 to new arbitary defined class 1 \n",
    "2. Remove unwanted variables ('a_7', 'a_21','a_28', 'a_33') as determined in data exploration step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Basic data preprocessing\n",
    "# Read in the dataset\n",
    "with open('dataset.csv') as fp:\n",
    "    data = pd.read_csv(fp)\n",
    "\n",
    "# dropping columns a_28, a_33, a_7 and a_21\n",
    "data.drop(columns=['a_28', 'a_33', 'a_21', 'a_7'], inplace=True)\n",
    "\n",
    "# Assigning new category to a_0 column for categories which are rare\n",
    "a_0_rare = [100, 200, 2500, 5426, 5430, 5431, 1334134, 3734780, 6197502, 6735581]\n",
    "data['a_0'][data['a_0'].isin(a_0_rare)] = 1\n",
    "\n",
    "Y = data['label']\n",
    "X = data.drop(columns=['label'])\n",
    "\n",
    "# creating variables for different feature types containing respect column names \n",
    "bool_features = list(X.select_dtypes(include='bool').columns)\n",
    "# converting boolean features to float type. Not really necessary as it can happen during model training also\n",
    "data[bool_features] = data[bool_features].astype(float)\n",
    "categorical_features = ['a_0', 'a_6', 'a_8', 'a_20', 'a_29', 'a_46', 'a_63', 'a_80', 'a_86', 'a_87'] # not including a_21 here as it is already dropped\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features + bool_features]\n",
    "\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the preprocessing and model building steps. We use Pipeline submodule from sklearn package to construct concise preprocessing and modeling steps. \n",
    "As mentioned above, SVM will require numerical features to be rescaled as it is sensitive to absolute values of the feature. No scaling is required for random forest\n",
    "\n",
    "For SVM, we use elastic_net penalty term. Elastic net is weighted combination of l1 penalty (lasso penalty) and l2(Ridge penalty). The weight is controlled by l1_ratio. if l1_ration is 0 then elasticnet is same as l2 penalty, for l1_ratio equal to 1 it is same as l1 penalty and between 0 and 1 it is mixture of both. Generally, elasticnet gives better performance than either l1 or l2 penalty  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining preprocessing pipeline\n",
    "# RobustScaler because the data has outliers\n",
    "svm_feature_preprocessing = Pipeline([('encoding',ColumnTransformer(transformers=[('one_hot_encoding', OneHotEncoder(), categorical_features)], remainder='passthrough')), ('feature_scaling', RobustScaler())])\n",
    "# No scaling required\n",
    "rf_feature_preprocessing = ColumnTransformer(transformers=[('one_hot_encoding', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "\n",
    "# Defining the classifier pipeline\n",
    "svm_pipeline = Pipeline([(\"feature_transformation\", svm_feature_preprocessing), ('svm', SGDClassifier(loss='hinge', penalty='elasticnet', class_weight='balanced', random_state=100, verbose=1))])\n",
    "rf_pipeline = Pipeline([(\"feature_transformation\", rf_feature_preprocessing), ('rf', RandomForestClassifier(class_weight='balanced', random_state=100, verbose=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter tuning is very important to get the model with the best fit. For SVM, there are two parameters which are needed to be fine tuned. Cost parameter alpha and l1_ratio for elastic net penalty. For RF we fine tune two parameters one is max depth of trees and number of features to be considered for each split.\n",
    "\n",
    "We use Gridsearch cross validation with 5 folds to find optimal parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyper parameters range\n",
    "parameters_svm = {'svm__alpha': (0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5), 'svm__l1_ratio': list(np.arange(0, 1.1, 0.1))}\n",
    "parameters_rf = {'rf__max_depth': (2, 4, 6, 8, 10, 12, 16, 20), 'rf__max_features': (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data between training and test set\n",
    "mskf = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=100)\n",
    "for train_idx, test_idx in mskf.split(X, Y):\n",
    "    X_train = X.iloc[train_idx]\n",
    "    Y_train = Y.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    Y_test = Y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimum hyper parameters for svm and random forest classifier\n",
    "opt_svm = GridSearchCV(svm_pipeline, parameters_svm, scoring='roc_auc', n_jobs=-1, cv=5, verbose=True)\n",
    "opt_rf = GridSearchCV(rf_pipeline, parameters_rf, scoring='roc_auc', n_jobs=-1, cv=5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training support vector machine classifier ...\n",
      "Fitting 5 folds for each of 88 candidates, totalling 440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 440 out of 440 | elapsed:  8.1min finished\n",
      "/home/dgxuser_layersvanguard/anaconda/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.14, NNZs: 166, Bias: -0.518259, T: 400000, Avg. loss: 1.565469\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.24, NNZs: 166, Bias: -0.530089, T: 800000, Avg. loss: 1.121651\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.22, NNZs: 166, Bias: -0.536795, T: 1200000, Avg. loss: 1.150267\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.14, NNZs: 166, Bias: -0.541975, T: 1600000, Avg. loss: 1.129852\n",
      "Total training time: 3.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.16, NNZs: 166, Bias: -0.545256, T: 2000000, Avg. loss: 0.804905\n",
      "Total training time: 4.05 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feature_transformation', Pipeline(memory=None,\n",
       "     steps=[('encoding', ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('one_hot_encoding', OneHotEncoder(categorical_features=None, categories=None,\n",
       "    ...om_state=100, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'svm__alpha': (0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5), 'svm__l1_ratio': [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training support vector machine classifier ...')\n",
    "opt_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random forest classifier ...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 25.9min finished\n",
      "/home/dgxuser_layersvanguard/anaconda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feature_transformation', ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('one_hot_encoding', OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknow...s='warn', n_jobs=None, oob_score=False,\n",
       "            random_state=100, verbose=1, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'rf__max_depth': (2, 4, 6, 8, 10, 12, 16, 20), 'rf__max_features': (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training random forest classifier ...')\n",
    "opt_rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The macro f1-score of svm classifier is 41.8%\n",
      "\n",
      "The macro f1-score of random forest classifier is 47.8%\n"
     ]
    }
   ],
   "source": [
    "# Measure the f1_score of svm and random forest model on test data\n",
    "opt_svm_f1 = metrics.f1_score(Y_test, opt_svm.predict(X_test), average='macro')\n",
    "opt_rf_f1 = metrics.f1_score(Y_test, opt_rf.predict(X_test), average='macro')\n",
    "\n",
    "print(\"The macro f1-score of svm classifier is %.1f%%\" % (100 * opt_svm_f1))\n",
    "print(\"\\nThe macro f1-score of random forest classifier is %.1f%%\" % (100 * opt_rf_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for svm classifier is\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.82     99655\n",
      "           1       0.01      0.72      0.02       345\n",
      "\n",
      "   micro avg       0.69      0.69      0.69    100000\n",
      "   macro avg       0.50      0.71      0.42    100000\n",
      "weighted avg       1.00      0.69      0.82    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for svm model\n",
    "print(\"The classification report for svm classifier is\\n\")\n",
    "print(metrics.classification_report(Y_test, opt_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for random forest classifier is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92     99655\n",
      "           1       0.02      0.73      0.03       345\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    100000\n",
      "   macro avg       0.51      0.79      0.48    100000\n",
      "weighted avg       1.00      0.86      0.92    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for random forest model\n",
    "print(\"The classification report for random forest classifier is\")\n",
    "print(metrics.classification_report(Y_test, opt_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might seem that we have very low precision for class 1 but it should be remembered that class 1 is rare class. For rare class main thing is how many of actual rare instances are identified by the model i.e. recall of the model. Since Random forest is giving better performance we will select random forest classifier as our final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(opt_rf.best_estimator_, 'randomforest_model.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
